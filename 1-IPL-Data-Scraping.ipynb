{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "1. https://www.espncricinfo.com\n",
    "2. https://medium.com/swlh/web-scraping-cricinfo-data-c134fce79a33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPLDataScrapper:\n",
    "\n",
    "    def __init__(self, year, data_dir='Data'):\n",
    "        self.domain = 'https://www.espncricinfo.com'\n",
    "        self.data_dir = data_dir\n",
    "        if year >= 2010 and year <= 2020:\n",
    "            self.year = year\n",
    "            if self.year >= 2014 and self.year <= 2015:\n",
    "                self.season_url = self.domain + '/scores/series/8048/season/{}/pepsi-indian-premier-league?view=results'.format(self.year)\n",
    "            else:\n",
    "                self.season_url = self.domain + '/scores/series/8048/season/{}/indian-premier-league?view=results'.format(self.year)\n",
    "        else:\n",
    "            raise ValueError(\"Scrapper is defined only for the seasons from 2010 to 2020\")\n",
    "\n",
    "\n",
    "    def __extract_match_urls(self):\n",
    "        season_page = requests.get(self.season_url)\n",
    "        if season_page.status_code == 200:\n",
    "            soup = BeautifulSoup(season_page.content, 'html.parser')\n",
    "            matches = soup.find_all(class_='col-md-8 col-16')\n",
    "            match_urls = []\n",
    "            for match in matches:\n",
    "                match_url = self.domain + match.find('a', href=True)['href']\n",
    "                match_urls.append(match_url)\n",
    "        else:\n",
    "            raise ValueError(\"Response status code: {}\".format(season_page.status_code))\n",
    "\n",
    "        return match_urls\n",
    "\n",
    "\n",
    "    def __extract_batsman_data(self, soup):\n",
    "        batsman_tables = soup.find_all(class_=\"table batsman\")\n",
    "        #assert len(batsman_tables) == 2\n",
    "\n",
    "        columns = ['name', 'wicket', 'runs', 'balls', 'duration', 'fours', 'sixes', 'strike_rate']\n",
    "        for inning, batsman_table in enumerate(batsman_tables, start=1):\n",
    "            rows = batsman_table.find_all('tr')\n",
    "            batsman_list = []\n",
    "            for i in range(1, len(rows), 2):\n",
    "                batsman_row = rows[i]\n",
    "                cells = batsman_row.find_all('td')\n",
    "                cells = [cell.text.strip() for cell in cells]\n",
    "\n",
    "                if cells[0] == 'Extras':\n",
    "                    row = ['Extras', 'Extras', cells[2], '0', '0', '0', '0', '0']\n",
    "                    batsman_list.append(row)\n",
    "                elif len(cells) > 7:\n",
    "                    row = cells\n",
    "                    batsman_list.append(row)\n",
    "                else:\n",
    "                    if cells[0].startswith('Did not bat: '):\n",
    "                        batsmen = [batsman.strip() for batsman in cells[0][len('Did not bat: '):].split(',')]\n",
    "                        for batsman in batsmen:\n",
    "                            row = [batsman, 'Did not bat', '0', '0', '0', '0', '0', '0']\n",
    "                            batsman_list.append(row)\n",
    "                \n",
    "            batsman_df = pd.DataFrame(batsman_list, columns=columns)\n",
    "            if inning == 1:\n",
    "                batsman_df_1 = batsman_df\n",
    "                batsman_df_1['inning'] = 1\n",
    "            elif inning == 2:  \n",
    "                batsman_df_2 = batsman_df\n",
    "                batsman_df_2['inning'] = 2\n",
    "\n",
    "        if len(batsman_tables) == 2:\n",
    "            batsman_df = pd.concat([batsman_df_1, batsman_df_2])\n",
    "        elif len(batsman_tables) == 1:\n",
    "            batsman_df = batsman_df_1\n",
    "        elif len(batsman_tables) == 0:\n",
    "            batsman_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        return batsman_df \n",
    "\n",
    "\n",
    "    def __extract_bowler_data(self, soup):\n",
    "        bowler_tables = soup.find_all(class_=\"table bowler\")\n",
    "        #assert len(bowler_tables) == 2\n",
    "\n",
    "        columns = ['name', 'overs', 'maidens', 'runs', 'wickets', 'economy', 'dots', 'fours', 'sixes', 'wides', 'no_balls']\n",
    "        for inning, bowler_table in enumerate(bowler_tables, start=1):\n",
    "            rows = bowler_table.find_all('tr')\n",
    "            bowler_list = []\n",
    "            for i in range(1, len(rows)):\n",
    "                bowler_row = rows[i]\n",
    "                cells = bowler_row.find_all('td')\n",
    "                cells = [cell.text.strip() for cell in cells]\n",
    "                row = cells\n",
    "                bowler_list.append(row)\n",
    "                \n",
    "            bowler_df = pd.DataFrame(bowler_list, columns=columns)\n",
    "            if inning == 1:\n",
    "                bowler_df_1 = bowler_df\n",
    "                bowler_df_1['inning'] = 1\n",
    "            elif inning == 2:  \n",
    "                bowler_df_2 = bowler_df\n",
    "                bowler_df_2['inning'] = 2\n",
    "            \n",
    "\n",
    "        if len(bowler_tables) == 2:\n",
    "            bowler_df = pd.concat([bowler_df_1, bowler_df_2])\n",
    "        elif len(bowler_tables) == 1:\n",
    "            bowler_df = bowler_df_1\n",
    "        elif len(bowler_tables) == 0:\n",
    "            bowler_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        return bowler_df\n",
    "\n",
    "\n",
    "    def __extract_meta_data(self, soup, match_id, match_url):\n",
    "\n",
    "        match_name, location, date, _ = soup.find(class_='desc text-truncate').get_text().split(',')\n",
    "        match_name = match_name.replace('/', ' and ').strip()\n",
    "        team_names = soup.find_all('a', class_='team-name')\n",
    "        team_names = [team_name.find('span').get('title').strip() for team_name in team_names]\n",
    "        score_runs = [score_run.text.strip() for score_run in soup.find_all('div', class_='score-run')]\n",
    "        match_details_table = soup.find(class_=\"w-100 table match-details-table\")\n",
    "        match_details_rows = match_details_table.find_all('tr')\n",
    "        match_details = {}\n",
    "        for i in range(1, len(match_details_rows)):\n",
    "            cells = match_details_rows[i].find_all('td')\n",
    "            key = cells[0].get_text().strip()\n",
    "            value = cells[1].get_text().strip()\n",
    "            match_details[key] = value\n",
    "\n",
    "        meta_data = {}\n",
    "        meta_data['year'] = self.year\n",
    "        meta_data['match_id'] = match_id\n",
    "        meta_data['match_name'] = match_name\n",
    "        meta_data['match_url'] = match_url\n",
    "        meta_data['location'] = location.strip()\n",
    "        meta_data['stadium'] = match_details_rows[0].find('a').get_text().strip()\n",
    "        meta_data['date'] = date.strip()\n",
    "        meta_data['toss'] = match_details.get('Toss', 'Not Found')\n",
    "        meta_data['team_1'] = team_names[0]\n",
    "        meta_data['team_2'] = team_names[1]\n",
    "        meta_data['team_1_score'] = score_runs[0]\n",
    "        meta_data['team_2_score'] = score_runs[1]\n",
    "        if soup.find('div', class_='score-extra-score'):\n",
    "            meta_data['team_2_final_status'] = soup.find('div', class_='score-extra-score').get_text().strip()\n",
    "        else:\n",
    "            meta_data['team_2_final_status'] = 'Not Found'\n",
    "        if soup.find(class_='best-player-name'):\n",
    "            meta_data['best_player'] = soup.find(class_='best-player-name').find('a').get_text().strip()\n",
    "        else:\n",
    "            meta_data['team_2_final_status'] = 'Not Found'\n",
    "        meta_data['summary'] = soup.find('div', class_='summary').get_text().strip()\n",
    "        meta_data['points'] = match_details.get('Points', 'Not Found')\n",
    "\n",
    "        return meta_data\n",
    "\n",
    "\n",
    "    def scrape(self):\n",
    "        self.match_urls = self.__extract_match_urls()[::-1]\n",
    "        self.season_dir = os.path.join(self.data_dir, str(self.year))\n",
    "        if not os.path.exists(self.season_dir):\n",
    "            os.mkdir(self.season_dir)\n",
    "        for match_id, match_url in tqdm(enumerate(self.match_urls, start=1), desc=\"Matches\", leave=False, total=len(self.match_urls)):\n",
    "            match_page = requests.get(match_url)\n",
    "            soup = BeautifulSoup(match_page.content, 'html.parser')\n",
    "            \n",
    "            match_name, location, date, _ = soup.find(class_='desc text-truncate').get_text().split(',')\n",
    "            match_name = match_name.replace('/', ' and ').strip()\n",
    "            \n",
    "            meta_data = self.__extract_meta_data(soup, match_id, match_url)\n",
    "            batsman_df = self.__extract_batsman_data(soup)\n",
    "            bowler_df = self.__extract_bowler_data(soup)\n",
    "            \n",
    "            match_dir = os.path.join(self.season_dir, match_name)\n",
    "            if not os.path.exists(match_dir):\n",
    "                os.mkdir(match_dir)\n",
    "\n",
    "            with open(os.path.join(match_dir, 'meta_data.json'), 'w') as fp:\n",
    "                json.dump(meta_data, fp, sort_keys=False, indent=4)\n",
    "            batsman_df.to_csv(os.path.join(match_dir, 'batsman_df.csv'), index=False)\n",
    "            bowler_df.to_csv(os.path.join(match_dir, 'bowler_df.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for year in tqdm(range(2010, 2020), 'Seasons'):\n",
    "#    scrapper = IPLDataScrapper(year=year)\n",
    "#    scrapper.scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Matches', max=7.0, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scrapper = IPLDataScrapper(year=2020)\n",
    "scrapper.scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
